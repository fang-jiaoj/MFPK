from utils import Mol_Tokenizer,get_dist_matrix,get_adj_matrix,molgraph_rep
from torch.utils.data import Dataset,DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
import numpy as np
import pandas as pd
import tqdm
import os
import collections
from UniMol import UniMolRepr
from functools import partial
from multiprocessing import Process,Queue,cpu_count,Lock
import lmdb
import pickle
import gc
import time

def process_single_smiles(smi_list, tokenizer_path, result_queue):
    """å¤„ç†å•ä¸ªåˆ†å­"""
    tokenizer = Mol_Tokenizer(tokenizer_path)  # åˆå§‹åŒ–ä¸€æ¬¡
    # unimol_encoder
    unimol = UniMolRepr(data_type='molecule',
                        remove_hs=False,
                        model_name='unimolv1',
                        model_size='84m',
                        use_gpu=True)
    batch_results = {}  # å­˜å‚¨å½“å‰æ‰¹æ¬¡çš„è®¡ç®—ç»“æœ
    smi_batch = []
    batch_size = 64  # **æ¯ 10000 ä¸ª SMILES æ‰¹é‡å­˜å…¥ LMDB**

    for smi in tqdm.tqdm(smi_list, desc='SMILES Processing...'):
        smi_batch.append(smi)

        if len(smi_batch) >= batch_size:
            try:
                # æ‰¹é‡è®¡ç®—
                batch_num_list, batch_edges, batch_cliques = zip(*[tokenizer.tokenize(s) for s in smi_batch])
                unimol_output = unimol.get_repr(smi_batch, return_atomic_reprs=True)

                for index, s in enumerate(smi_batch):
                    result = {
                        "unimol_embeds": unimol_output['atomic_reprs'][index],
                        "unimol_embedds_mask": unimol_output['atomic_mask'][index],
                        'adj_matrix': get_adj_matrix(batch_num_list[index], batch_edges[index]),
                        'dist_matrix': get_dist_matrix(batch_num_list[index], batch_edges[index]),
                        'nums_list': batch_num_list[index],
                        'cliques': batch_cliques[index],
                        'edges': batch_edges[index],
                        'single_dict': molgraph_rep(s, batch_cliques[index])
                    }
                    batch_results[s] = result  # å…ˆå°†ç»“æœå­˜å…¥æœ¬åœ°çš„å­—å…¸

            except Exception as e:
                print(f"Error processing {smi}: {e}")
                for smi in smi_batch:
                    if smi not in batch_results:
                        batch_results[smi] = {}

            try:
                result_queue.put(batch_results)
                batch_results = {}  # æ¸…ç©ºæœ¬åœ°ç¼“å­˜
            except Exception as e:
                print(f"[Queue Error] Failed to put batch into queue: {e}")

            smi_batch = []  # æ¸…ç©ºbatch
        torch.cuda.empty_cache()  # âœ… é‡Šæ”¾æ˜¾å­˜
        gc.collect()

    # å†™å…¥å‰©ä½™æ•°æ®
    if smi_batch:
        try:
            batch_num_list, batch_edges, batch_cliques = zip(*[tokenizer.tokenize(s) for s in smi_batch])
            unimol_output = unimol.get_repr(smi_batch, return_atomic_reprs=True)

            for index, smi in enumerate(smi_batch):
                result = {
                    "unimol_embeds": unimol_output['atomic_reprs'][index],
                    "unimol_embedds_mask": unimol_output['atomic_mask'][index],
                    'adj_matrix': get_adj_matrix(batch_num_list[index], batch_edges[index]),
                    'dist_matrix': get_dist_matrix(batch_num_list[index], batch_edges[index]),
                    'nums_list': batch_num_list[index],
                    'cliques': batch_cliques[index],
                    'edges': batch_edges[index],
                    'single_dict': molgraph_rep(smi, batch_cliques[index])
                }
                batch_results[smi] = result

        except Exception as e:
            print(f"[Error] Last batch processing failed: {e}")
            for smi in smi_batch:
                if smi not in batch_results:
                    batch_results[smi] = {}

        try:
            result_queue.put(batch_results)
        except Exception as e:
            print(f"[Queue Error] Failed to put final batch: {e}")

    gc.collect()
    time.sleep(1)  # **é¿å…è¿›ç¨‹ç«äº‰**


def save_dict_to_lmdb(result_queue, lmdb_path, lock):
    """å°†æ•°æ®å­˜å‚¨åˆ°LMDBå­—å…¸ä¸­"""
    env = lmdb.open(lmdb_path, map_size=1099511627776, sync=True, metasync=True, writemap=True)  # åˆ›å»ºLMDBç¯å¢ƒ

    while True:
        batch_result = result_queue.get()
        if batch_result is None:
            break

        with lock:
            with env.begin(write=True) as txn:
                for smi, result in batch_result.items():
                    # print(f"SMILES:{smi},dict:{result}")
                    txn.put(smi.encode('utf-8'), pickle.dumps(result))

    env.sync()  # ç¡®ä¿æ•°æ®æŒä¹…åŒ–
    env.close()
    print(f"æ•°æ®å­˜å‚¨å·²å®Œæˆï¼š{lmdb_path}")

    # é‡æ–°æ‰“å¼€æ•°æ®åº“ï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æˆåŠŸå†™å…¥
    env = lmdb.open(lmdb_path, readonly=True)
    with env.begin() as txn:
        with txn.cursor() as cursor:
            for key, value in cursor:
                value = pickle.loads(value)
                print(f"ğŸ“– è¯»å– SMILES: {key.decode('utf-8')}, Map Dict Keys: {value.keys()}æˆåŠŸï¼")

    env.close()


def calc_all_molecule_dict(data_path, lmdb_path, num_workers=4):
    """å°†æ‰€æœ‰SMILESå¹¶å­˜å…¥LMDB"""
    pretrain_data = pd.read_csv(data_path)
    pretrain_smi = pretrain_data.iloc[:, 0].values.tolist()
    pretrain_smi_length = len(pretrain_smi)

    tokenizer_path = '../Data/vocab_token.json'
    # fintune_tokenizer_path = r'.\multi_datasets\vocab_token.json'
    result_queue = Queue()  # è¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    lock = Lock()  # **ä¿è¯å¤šè¿›ç¨‹å­˜å‚¨æ—¶ä¸å†²çª**

    processes = []
    smi_list = []
    index = 0
    for i in range(num_workers):
        batch_smi = pretrain_smi_length // num_workers
        smi_list.append(pretrain_smi[index:index + batch_smi])
        index += batch_smi
    if index < pretrain_smi_length:
        smi_list[-1].extend(pretrain_smi[index:])

    # å¯åŠ¨è®¡ç®—è¿›ç¨‹
    for j in range(num_workers):
        p = Process(target=process_single_smiles, args=(smi_list[j], tokenizer_path, result_queue))
        processes.append(p)
        p.start()

    # å¯åŠ¨LMDBè¿›ç¨‹
    write_process = Process(target=save_dict_to_lmdb, args=(result_queue, lmdb_path, lock))
    write_process.start()

    for p in processes:
        p.join()

    # all_molecule_dict = {}
    # with tqdm.tqdm(total=pretrain_smi_length,desc='Processing SMILES') as pbar:
    #     for _ in range(pretrain_smi_length):
    #         smi,result = result_queue.get()
    #         all_molecule_dict[smi] =  result
    #         pbar.update(1)

    # for p in processes:
    #     p.join()
    #
    # save_dict_to_lmdb(all_molecule_dict,lmdb_path)
    # **å‘é€ç»“æŸä¿¡å·ï¼Œé€šçŸ¥å†™å…¥è¿›ç¨‹é€€å‡º**
    result_queue.put(None)
    write_process.join()


if __name__ == '__main__':
    # data_path = '/HOME/scz0bnb/run/project_2/My_model_V3/pretrain_datasets/pre_pretrain_data_random.csv'
    # lmdb_path = '/HOME/scz0bnb/run/project_2/My_model_V3/pretrain_datasets/all_molecule_dict_unimol.lmdb'
    fintune_path = r'../Data/MTL_data/multitask_datasets.csv'
    fintune_lmdb_path = r'../Data/all_molecule_dict_unimol.lmdb'
    calc_all_molecule_dict(fintune_path, fintune_lmdb_path)